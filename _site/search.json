[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Devos Blog",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2026\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nHow AI Generates Images: A Simple Explanation\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2026\n\n\nDevon Vorster\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/my-first-post/index.html",
    "href": "posts/my-first-post/index.html",
    "title": "How AI Generates Images: A Simple Explanation",
    "section": "",
    "text": "With the popularity and advancement of AI rapidly accelerating, artificial intelligence has become a part of daily life for many people. In particular, advances in image-generating AI models have been an especially exciting area. Diffusion models, named after the physical process they are inspired by, are currently the most widely used and successful tools for generating images with AI. Diffusion models are complex, but in this post I will attempt to break them down in a more manageable way to help explain how they work. At a high level, diffusion models can be divided into two stages: a forward stage and a backward stage. Surprisingly, these processes can be explained quite simply by likening them to a game of telephone.\n\n\nFor anyone who has never played, telephone starts when one person whispers a phrase or sentence into another person’s ear. That phrase is then passed to the next person, and the next, until it reaches the last person in line. The goal of the game is to keep the phrase as close to the original as possible. Inevitably, as the phrase moves down the line, people make mistakes and it gets morphed into something completely different, often nonsensical. This is what makes the game enjoyable — you never really know what will happen at the end. For example, the phrase “a cat jumps over the moon and runs across the stars” might end up as “the cat’s stars are jumps and the moon is over.”\n\n\n\nFigure 1: A game of telephone where carrot turns into elephant ducks and a bicycle. Generated by Google Gemini (Nano Banana version), January 16, 2026\n\n\nA diffusion model works in a somewhat similar way. We start with an image and iteratively add random noise — small random changes to the pixels — until the image becomes pure static, like the static on a TV. This noise plays a role similar to the changes that happen as a phrase moves from person to person in the game of telephone. Each step makes the image a little stranger, until eventually it is no longer recognizable at all. Unlike in the game, however, these changes are not caused by misunderstanding; they are deliberately added randomness.\n\n\n\nSo why would we want to destroy an image like this? Let’s return to our telephone analogy. Imagine playing a billion games of telephone, and for each game there is a single observer. This observer is not part of the game, but they can hear the phrase at every step as it moves down the line. Over time, they become very good at understanding the kinds of changes that typically happen from one step to the next. Eventually, they learn how to predict what the phrase likely sounded like before the most recent change.\n\n\n\nFigure 2: Diffusion process.\n\n\nIf we now give this observer the final, altered phrase, they could work backwards step by step, predicting how to undo each change until they reach something very close to the original sentence. Importantly, the observer does not know the previous phrases. The best they can do is make incremental predictions about what likely versions of the earlier stages might have been. If they make good predictions, they may get close to the original phrase. This is essentially how diffusion models work as well. During training, the model learns similar patterns — specifically, how to predict the noise that was added at each step of the process. Then, when generating an image, the model starts from pure noise and repeatedly removes small amounts of it. Step by step, random static turns into structure, and structure turns into an image.\nWhen you enter a prompt into an image generator — “an orange cat,” for example — the model begins with static and gradually works backwards, predicting how to remove noise a little bit at a time in a way that results in something resembling an orange cat. Just like the observer, the model doesn’t actually know the previous stages. It only sees the static. The best it can do is make predictions about what likely versions of those earlier stages might have been, based on its learned patterns and the description “an orange cat” (more on this later). This is why AI-generated images are sometimes not quite right: the model cannot always return to something that perfectly resembles an actual orange cat, just as the observer cannot always recover the original phrase.\n\n\n\nThat explanation raises another question: how does the model know how to make a cat when asked for a cat, and a dog when asked for a dog?\nThe answer lies in something called text embeddings.\nDuring training, each image is paired with a text description, such as an image of an orange cat labeled “an orange cat.”This text is converted into a text embedding — a numerical representation of the meaning of the description that can be passed to the model. The image and its text embedding are then processed together. As noise is added to images, the early steps still reflect the original image’s structure. The model learns how these noisy images relate to their associated text descriptions via the text embeddings. That is, it learns different ways that an image of an orange cat tends to break down under noise compared to how an image of a brown dog breaks down. These differences are part of the “learned patterns” discussed earlier, and they help steer the model toward generating different images.\nWhen you later type a prompt like “an orange cat” or “a fantasy castle with a dragon,” that text is converted into a text embedding and fed into the model at every step of the denoising process. The text embedding gently steers each step of noise removal toward images that match your prompt. Again, the model does not know the previous stages; the best it can do is make predictions about what likely versions of those stages might have been. The information encoded in the text embedding for “an orange cat” helps guide those predictions toward something that looks like an orange cat rather than a brown dog.\n\n\n\nFigure 2: An image from Google Gemini when prompted for a “realistic orange cat,” generated by Google Gemini (Nano Banana version), January 16, 2026.\n\n\nThis process is not perfect, and models still make mistakes. However, by using diffusion models, we turn the hard problem of creating an image from nothing into the much easier task of removing a little noise at a time. In this way, diffusion models are able to generate remarkably accurate images from random static."
  },
  {
    "objectID": "posts/my-first-post/index.html#moving-forwards",
    "href": "posts/my-first-post/index.html#moving-forwards",
    "title": "How AI Generates Images: A Simple Explanation",
    "section": "",
    "text": "For anyone who has never played, telephone starts when one person whispers a phrase or sentence into another person’s ear. That phrase is then passed to the next person, and the next, until it reaches the last person in line. The goal of the game is to keep the phrase as close to the original as possible. Inevitably, as the phrase moves down the line, people make mistakes and it gets morphed into something completely different, often nonsensical. This is what makes the game enjoyable — you never really know what will happen at the end. For example, the phrase “a cat jumps over the moon and runs across the stars” might end up as “the cat’s stars are jumps and the moon is over.”\n\n\n\nFigure 1: A game of telephone where carrot turns into elephant ducks and a bicycle. Generated by Google Gemini (Nano Banana version), January 16, 2026\n\n\nA diffusion model works in a somewhat similar way. We start with an image and iteratively add random noise — small random changes to the pixels — until the image becomes pure static, like the static on a TV. This noise plays a role similar to the changes that happen as a phrase moves from person to person in the game of telephone. Each step makes the image a little stranger, until eventually it is no longer recognizable at all. Unlike in the game, however, these changes are not caused by misunderstanding; they are deliberately added randomness."
  },
  {
    "objectID": "posts/my-first-post/index.html#learning-to-go-backwards",
    "href": "posts/my-first-post/index.html#learning-to-go-backwards",
    "title": "How AI Generates Images: A Simple Explanation",
    "section": "",
    "text": "So why would we want to destroy an image like this? Let’s return to our telephone analogy. Imagine playing a billion games of telephone, and for each game there is a single observer. This observer is not part of the game, but they can hear the phrase at every step as it moves down the line. Over time, they become very good at understanding the kinds of changes that typically happen from one step to the next. Eventually, they learn how to predict what the phrase likely sounded like before the most recent change.\n\n\n\nFigure 2: Diffusion process.\n\n\nIf we now give this observer the final, altered phrase, they could work backwards step by step, predicting how to undo each change until they reach something very close to the original sentence. Importantly, the observer does not know the previous phrases. The best they can do is make incremental predictions about what likely versions of the earlier stages might have been. If they make good predictions, they may get close to the original phrase. This is essentially how diffusion models work as well. During training, the model learns similar patterns — specifically, how to predict the noise that was added at each step of the process. Then, when generating an image, the model starts from pure noise and repeatedly removes small amounts of it. Step by step, random static turns into structure, and structure turns into an image.\nWhen you enter a prompt into an image generator — “an orange cat,” for example — the model begins with static and gradually works backwards, predicting how to remove noise a little bit at a time in a way that results in something resembling an orange cat. Just like the observer, the model doesn’t actually know the previous stages. It only sees the static. The best it can do is make predictions about what likely versions of those earlier stages might have been, based on its learned patterns and the description “an orange cat” (more on this later). This is why AI-generated images are sometimes not quite right: the model cannot always return to something that perfectly resembles an actual orange cat, just as the observer cannot always recover the original phrase."
  },
  {
    "objectID": "posts/my-first-post/index.html#text-embeddings",
    "href": "posts/my-first-post/index.html#text-embeddings",
    "title": "How AI Generates Images: A Simple Explanation",
    "section": "",
    "text": "That explanation raises another question: how does the model know how to make a cat when asked for a cat, and a dog when asked for a dog?\nThe answer lies in something called text embeddings.\nDuring training, each image is paired with a text description, such as an image of an orange cat labeled “an orange cat.”This text is converted into a text embedding — a numerical representation of the meaning of the description that can be passed to the model. The image and its text embedding are then processed together. As noise is added to images, the early steps still reflect the original image’s structure. The model learns how these noisy images relate to their associated text descriptions via the text embeddings. That is, it learns different ways that an image of an orange cat tends to break down under noise compared to how an image of a brown dog breaks down. These differences are part of the “learned patterns” discussed earlier, and they help steer the model toward generating different images.\nWhen you later type a prompt like “an orange cat” or “a fantasy castle with a dragon,” that text is converted into a text embedding and fed into the model at every step of the denoising process. The text embedding gently steers each step of noise removal toward images that match your prompt. Again, the model does not know the previous stages; the best it can do is make predictions about what likely versions of those stages might have been. The information encoded in the text embedding for “an orange cat” helps guide those predictions toward something that looks like an orange cat rather than a brown dog.\n\n\n\nFigure 2: An image from Google Gemini when prompted for a “realistic orange cat,” generated by Google Gemini (Nano Banana version), January 16, 2026.\n\n\nThis process is not perfect, and models still make mistakes. However, by using diffusion models, we turn the hard problem of creating an image from nothing into the much easier task of removing a little noise at a time. In this way, diffusion models are able to generate remarkably accurate images from random static."
  }
]